<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>holozoo</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">holozoo</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="holozoo"><a class="header" href="#holozoo">holozoo</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="calculating-array-sum-with-speed-of-l1-cache"><a class="header" href="#calculating-array-sum-with-speed-of-l1-cache">Calculating array sum with speed of L1 cache</a></h1>
<h2 id="problem-statement"><a class="header" href="#problem-statement">Problem statement</a></h2>
<p>Calculate sum of array of numbers. Datatype can be either of <code>int32_t</code>, <code>int64_t</code>, <code>float</code>, <code>double</code>.
Do this on x86-64. Do it as fast as possible.
Array size can be arbitrary, but in range 1024-16384.</p>
<h2 id="tldr"><a class="header" href="#tldr">TLDR</a></h2>
<ul>
<li>Use AVX-512</li>
<li>Use as much accumulator variables as possible</li>
</ul>
<h2 id="in-search-of-fastest-code"><a class="header" href="#in-search-of-fastest-code">In search of fastest code</a></h2>
<h3 id="naive-solution"><a class="header" href="#naive-solution">Naive solution</a></h3>
<pre><code class="language-cpp">float sum(size_t count, const float *data) {
    float accum = 0;
    for (size_t i = 0; i &lt; count; ++i)
        accum += data[i];
    return accum;
}
</code></pre>
<h4 id="gcc"><a class="header" href="#gcc">GCC</a></h4>
<p>If we compile with <code>gcc -O2</code>, we got the following code in main loop.
It has two additions to the <code>xmm0</code> register, which serves as sum accumulator.
Notice that there are two additions: GCC decided to unroll the loop to process 2 elements per iteration.</p>
<pre><code class="language-x86asm">.L3:
        vaddss  xmm0, xmm0, DWORD PTR [rsi]
        add     rsi, 8
        vaddss  xmm0, xmm0, DWORD PTR [rsi-4]
        cmp     rsi, rax
        jne     .L3
</code></pre>
<h4 id="clang"><a class="header" href="#clang">Clang</a></h4>
<p>However, if we compile with <code>clang -O2</code>, we get the following loop:</p>
<pre><code class="language-x86asm">.LBB0_9:
        vaddss  xmm0, xmm0, dword ptr [rsi + 4*rcx]
        vaddss  xmm0, xmm0, dword ptr [rsi + 4*rcx + 4]
        vaddss  xmm0, xmm0, dword ptr [rsi + 4*rcx + 8]
        vaddss  xmm0, xmm0, dword ptr [rsi + 4*rcx + 12]
        vaddss  xmm0, xmm0, dword ptr [rsi + 4*rcx + 16]
        vaddss  xmm0, xmm0, dword ptr [rsi + 4*rcx + 20]
        vaddss  xmm0, xmm0, dword ptr [rsi + 4*rcx + 24]
        vaddss  xmm0, xmm0, dword ptr [rsi + 4*rcx + 28]
        add     rcx, 8
        cmp     rdi, rcx
        jne     .LBB0_9
</code></pre>
<p>Clang unrolls loop to process 8 items per iteration.</p>
<h4 id="comparison"><a class="header" href="#comparison">Comparison</a></h4>
<p>Let's plug this code into <a href="https://uica.uops.info">uiCA</a> to calculate loop throughput.</p>
<blockquote>
<p>uiCA is throughput prediction tool described in <a href="https://arxiv.org/pdf/2107.14210">uiCA: Accurate Throughput Prediction of
Basic Blocks on Recent Intel Microarchitectures</a></p>
</blockquote>
<p>GCC:</p>
<pre><code>Throughput (in cycles per iteration): 8.00
Bottleneck: Dependencies

The following throughputs could be achieved if the given property were the only bottleneck:

  - DSB: 1.00
  - Issue: 1.00
  - Ports: 1.00
  - Dependencies: 8.00
</code></pre>
<p>Clang:</p>
<pre><code>Throughput (in cycles per iteration): 32.00
Bottleneck: Dependencies

The following throughputs could be achieved if the given property were the only bottleneck:

  - DSB: 1.67
  - Issue: 4.50
  - Ports: 4.00
  - Dependencies: 32.00
</code></pre>
<p>GCC has predicted throughput of 8 cycles/iteration. It processes 2 * 4 = 8 bytes/iteration. So, it processes 1 byte/cycle.</p>
<p>Clang has predicated throughput of 32 cycles/iteration. It processes 8 * 4 = 32 bytes/iteration. So, it processes 1 byte/cycle.</p>
<p>The throughput is the same.</p>
<p>We can plug the code that does not unroll the loop, and see that we would get the same 1 byte/cycle.</p>
<p>What's the deal here?
uiCA tells us in the second line:</p>
<pre><code>Bottleneck: Dependencies
</code></pre>
<p>Modern CPUs are pipelined, and multiple iterations of loop can be running simultaneously. That works if they do not have data dependencies (also called <a href="https://en.wikipedia.org/wiki/Hazard_(computer_architecture)">data hazards</a>).</p>
<p>uiCA provides us with pipeline view. We can see that each next addition cannot execute <code>E</code> until previous finished running (this is indicated by <code>D</code> - dependency).
So, loop unrolling does not matter because all the additions are waiting on the same register to be updated.
<img src="simd/image.png" alt="alt text" /></p>
<h4 id="improving-compiler-result"><a class="header" href="#improving-compiler-result">Improving compiler result</a></h4>
<p>We see that both GCC and Clang generate code that adds all array elements to a single accumulator sequentially.
This happens because compilers assume that floating-point addition is not commutative (a + b != b + a for any a, b) under the IEEE standard.
Compilers have option <code>-ffast-math</code> to relax this a bit.</p>
<blockquote>
<p>Programs compiled with <code>-ffast-math</code> can produce results different from naive solution, and often much less precise. This is not our concern, since we try to do summation as fast as possible. But to do correct summation there are techniques like <a href="https://en.wikipedia.org/wiki/Pairwise_summation">Pairwise summation</a> used, for example, in <a href="https://github.com/apache/arrow/blob/37f62d0bc5f4d22e7194947963b445225b984558/cpp/src/arrow/compute/kernels/aggregate_internal.h#L144">Apache Arrow</a>.</p>
</blockquote>
<h4 id="gcc-1"><a class="header" href="#gcc-1">GCC</a></h4>
<p>Compiling the code using GCC with <code>-ffast-math</code> we get the same result.</p>
<blockquote>
<p>We could get GCC to generate better code with <code>-funroll-loops</code> and <code>-fvariable-expansion-in-unroller</code> flags (But if compilers were any good at their job we would have no need in these flags).</p>
</blockquote>
<h4 id="clang-1"><a class="header" href="#clang-1">Clang</a></h4>
<p>Compiling the code using Clang with <code>-ffast-math</code> we get the following code:</p>
<pre><code class="language-x86asm">.LBB0_5:
        vaddps  ymm0, ymm0, ymmword ptr [rsi + 4*rcx]
        vaddps  ymm1, ymm1, ymmword ptr [rsi + 4*rcx + 32]
        vaddps  ymm2, ymm2, ymmword ptr [rsi + 4*rcx + 64]
        vaddps  ymm3, ymm3, ymmword ptr [rsi + 4*rcx + 96]
        add     rcx, 32
        cmp     rax, rcx
        jne     .LBB0_5
</code></pre>
<p>First thing to notice is that Clang generates code using <code>ymm</code> registers coming from <a href="https://en.wikipedia.org/wiki/Advanced_Vector_Extensions">AVX</a> instruction set.
Secondly, it uses <code>ps</code> versions of addition - adding together <code>packed scalars</code>. Each <code>vaddps</code> instruction adds together 8 float values in parallel.
We can see that clang reduced the unroll count to 4 (previously being 8), and uses 4 distinct accumulators.
In the function epilogue these accumulators are added together to produce the final result.</p>
<p>What uiCA tells us about this code?</p>
<pre><code class="language-x86asm">Throughput (in cycles per iteration): 4.02
Bottleneck: Dependencies

The following throughputs could be achieved if the given property were the only bottleneck:

  - LSD: 1.33
  - Issue: 2.00
  - Ports: 2.00
  - Dependencies: 4.00
</code></pre>
<p><img src="simd/image-1.png" alt="alt text" /></p>
<blockquote>
<p>Notice that uiCA still tells us that this code is dependency-bound</p>
</blockquote>
<p>Code has predicated throughput of 4 cycles/iteration. It processes 8 * 4 * 4 = 128 bytes/iteration. So, it processes 32 bytes/cycle.
This is 32 times more than previous version!</p>
<h3 id="manual-vectorization"><a class="header" href="#manual-vectorization">Manual vectorization</a></h3>
<p>What we got as result from investigating compiler output is actually obvious (or becomes obvious, if you spend too much free time on problems like this):</p>
<ul>
<li>If you want to compute faster, use SIMD instructions.</li>
<li>Use more accumulators to remove data dependencies.</li>
</ul>
<p>This is manually vectorized addition loop, using SIMD instructions from AVX instruction set:</p>
<pre><code class="language-cpp">double hsum_double_avx(__m256d v) 
{
    __m128d vlow  = _mm256_castpd256_pd128(v);
    __m128d vhigh = _mm256_extractf128_pd(v, 1); // high 128
            vlow  = _mm_add_pd(vlow, vhigh);     // reduce down to 128
    __m128d high64 = _mm_unpackhi_pd(vlow, vlow);
    return  _mm_cvtsd_f64(_mm_add_sd(vlow, high64));  // reduce to scalar
}

double f64_arr_sum_w4(const double *arr, int count)
{
    double sum = 0.0;
    __m256d accum0 = _mm256_setzero_pd();
    for (int i = 0; i &lt; count - 3; i += 4) {
        __m256d v0 = _mm256_load_pd(&amp;arr[i]);
        accum0 = _mm256_add_pd(accum0, v0);
    } 
    sum = hsum_double_avx(accum0);
    return sum;
}
</code></pre>
<blockquote>
<p>What happens in <code>hsum_double_avx</code> is not very important. You can read more about the function used <a href="https://stackoverflow.com/questions/49941645/get-sum-of-values-stored-in-m256d-with-sse-avx">in this StackOverflow answer</a>.</p>
</blockquote>
<p>If we use 4 accumulators here, and add them up in the end, we would get pretty much the same code as clang generates.</p>
<p>The question becomes, how many accumulators should we use?</p>
<p>I wrote the this function for all datatypes of interest (<code>int32_t</code>, <code>int64_t</code>, <code>float</code>, <code>double</code>), with accumulator count varying from 1 to 16 (as there are 16 ymm registers, and 32 zmm registers with AVX-512).
I did this using AVX and AVX-512 instructions.</p>
<h4 id="benchmarks"><a class="header" href="#benchmarks">Benchmarks</a></h4>
<p>These are results of benchmarks done using <a href="https://github.com/google/benchmark">google benchmark</a>.
I used a machine with <a href="https://en.wikichip.org/wiki/intel/microarchitectures/cascade_lake">Cascade Lake</a> CPU.
Cascade Lake has good support of AVX-512 and does not suffer from underclocking as much as Skylake-based CPUs did.
I used array size of 1024.</p>
<p><code>double</code>:</p>
<div class="table-wrapper"><table><thead><tr><th></th><th>1 acc</th><th>2 acc</th><th>4 acc</th><th>8 acc</th><th>16 acc</th></tr></thead><tbody>
<tr><td>AVX2</td><td>259ns</td><td>160ns</td><td>87ns</td><td>56ns</td><td>56ns</td></tr>
<tr><td>AVX512</td><td>131ns</td><td>88ns</td><td>60ns</td><td>57ns</td><td></td></tr>
</tbody></table>
</div>
<p><code>float</code>:</p>
<div class="table-wrapper"><table><thead><tr><th></th><th>1 acc</th><th>2 acc</th><th>4 acc</th><th>8 acc</th><th>16 acc</th></tr></thead><tbody>
<tr><td>AVX2</td><td>103ns</td><td>69ns</td><td>39ns</td><td>31ns</td><td>31ns</td></tr>
<tr><td>AVX512</td><td>64ns</td><td>44ns</td><td>34ns</td><td>31ns</td><td></td></tr>
</tbody></table>
</div>
<p><code>int64_t</code>:</p>
<div class="table-wrapper"><table><thead><tr><th></th><th>1 acc</th><th>2 acc</th><th>4 acc</th><th>8 acc</th></tr></thead><tbody>
<tr><td>AVX2</td><td>80ns</td><td>74ns</td><td>74ns</td><td></td></tr>
<tr><td>AVX512</td><td>51ns</td><td>50ns</td><td>47ns</td><td>47ns</td></tr>
</tbody></table>
</div>
<p><code>int32_t</code>:</p>
<div class="table-wrapper"><table><thead><tr><th></th><th>1 acc</th><th>2 acc</th><th>4 acc</th><th>8 acc</th></tr></thead><tbody>
<tr><td>AVX2</td><td>131ns</td><td>81ns</td><td>81ns</td><td></td></tr>
<tr><td>AVX512</td><td>36ns</td><td>21ns</td><td>22ns</td><td>23ns</td></tr>
</tbody></table>
</div>
<h4 id="conclusion"><a class="header" href="#conclusion">Conclusion</a></h4>
<p>AVX-512 is superior in speed. Always.
AVX-512 instructions take the same time to decode and execute, but are able to process more elements, as well as remove pressure from memory subsystem, issuing less requests.</p>
<p>We can see that all datatypes converge to the same time as number of accumulators increases. We can say that speed does not degrades with greater accumulator count, but it stops improving from certain point either.
We can also see that using 8 accumulators for doing sum of <code>float</code> array using AVX (what we did in first part of post) is faster with 8 accumulators, than with 4 (as clang generated). But this difference is unimportant.
uiCA would say that using 8 accumulators instead of 4 would provide twice the throughput, but it is clearly not the case.</p>
<blockquote>
<p>One of the things I noted for myself - is that throughput predictors are unable to handle memory-bound problems like this well. They usually provide approximate results, and sometimes outright incorrect.</p>
</blockquote>
<p>The most important question here - what is the limit? Why code stops getting faster?</p>
<p>We can try to guess. Let's calculate the time throughput for <code>double</code> and <code>float</code> sums using AVX-512.
They take 56 and 31 ns respectively. They process 8 * 1024 = 8192 and 4 * 1024 = 4096 bytes respectively.
This is 146 bytes/ns and 132 bytes/ns throughput. This is about 140 GB/s.</p>
<p>This sounds like a lot. We can checks <a href="https://travisdowns.github.io/blog/2019/06/11/speed-limits.html">Performance speed limits</a>, or just google - what is the speed of memory for fastest access.
Array data is accessed through L1 cache. L1 on Cascade Lake works 64 bytes/per cycle.
Knowing that our CPU operates at about 3GHz, we get 182 GB/s of maximum possible throughput.</p>
<p>Our code hit the L1 cache speed limit - it can't get any faster. This means that we process data faster than we read it, so, as I see it, this is satisfactory result.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="unpacking-8-bits-to-8-bools"><a class="header" href="#unpacking-8-bits-to-8-bools">Unpacking 8 bits to 8 bools</a></h1>
<h2 id="problem-statement-1"><a class="header" href="#problem-statement-1">Problem statement</a></h2>
<p>8 bits stored in <code>uint8_t</code> variable. Unpack each bit into distinct <code>bool</code> values, stored continuously in array (first bit goes into first bool and so on).
Do it as fast as possible.</p>
<h2 id="tldr-1"><a class="header" href="#tldr-1">TLDR</a></h2>
<ul>
<li>Use <code>PDEP</code> instruction from <code>BMI2</code> if available</li>
<li>You can use bithack described here to do the same thing</li>
<li>Ask yourself if you really need to do this because there is probably a better solution</li>
</ul>
<h2 id="in-search-of-fastest-code-1"><a class="header" href="#in-search-of-fastest-code-1">In search of fastest code</a></h2>
<h3 id="naive-solution-1"><a class="header" href="#naive-solution-1">Naive solution</a></h3>
<pre><code class="language-cpp">void unpack(uint8_t bits, bool bools[8]) {
    for (int i = 0; i &lt; 8; ++i)
        bools[i] = !!(bits &amp; (1 &lt;&lt; i));
}
</code></pre>
<p>This is clearly very inefficient. I wouldn't go into much detail to explain why it is so, so let's just go to better versions.</p>
<h3 id="using-bmi2"><a class="header" href="#using-bmi2">Using BMI2</a></h3>
<p>X86-64 has a number of ISA extensions. One of them is <a href="https://en.wikipedia.org/wiki/X86_Bit_manipulation_instruction_set#BMI2_(Bit_Manipulation_Instruction_Set_2)">BMI2</a> instruction set.
One of the instructions provided is <code>PDEP</code>: <a href="https://www.felixcloutier.com/x86/pdep">parallel bits deposit</a>.</p>
<p>It takes two 32-bit (or two 64-bit) operands and stores first n bits from first operand into destination using n positions selected with second operand, indicated with set bits.
This means that we can select first n bits and store in arbitrary n places in destination, provided that order stays the same.</p>
<p><img src="simd/image-2.png" alt="alt text" /></p>
<blockquote>
<p>You can find code for software implementation <a href="https://stackoverflow.com/questions/77834169/what-is-a-fast-fallback-algorithm-which-emulates-pdep-and-pext-in-software">here</a>.</p>
</blockquote>
<p>We can use it to do what we want with the following code:</p>
<pre><code class="language-cpp">void unpack(uint8_t bits, bool bools[8]) {
    uint64_t bools64 = _pdep_u64(bmask, 0x0101010101010101ul);
    memcpy(bools, &amp;bools64, sizeof(bools64));
}
</code></pre>
<p>We use <code>PDEP</code> here to select first 8 bits from <code>bits</code> and put then in places given with <code>0x0101010101010101ul</code>. These are first bits in each 8-bit part of 64-register, which equates to how bools are stored in memory.</p>
<p>Because bits are stored in byte in little-endian format, the resulting <code>bools64</code> is also little-endian. First bit goes to first byte, and order is not reversed.</p>
<p>What's the catch? BMI2 is not available on all machines, and it is infamously slow on AMD.</p>
<p>We can check <a href="https://uops.info/table.html">uops.info</a>:</p>
<div class="table-wrapper"><table><thead><tr><th></th><th>Skylake</th><th>Ice Lake</th><th>Zen2</th><th>Zen3</th></tr></thead><tbody>
<tr><td>Latency</td><td>3</td><td>3</td><td>18</td><td>3</td></tr>
<tr><td>TP</td><td>1</td><td>1</td><td>19</td><td>1</td></tr>
</tbody></table>
</div>
<p>Zen2 is 2019 architecture (same as Ice Lake), and it is quite possible to find machines that have this horrendously implemented <code>PDEP</code>.</p>
<h3 id="bithack"><a class="header" href="#bithack">Bithack</a></h3>
<p>Bithacks come to the rescue. As it turns out, it is possible to do required operation using nothing but arithmetic and bit operations without any array accesses.</p>
<p>Bithack is based on idea that we can use multiplication to place required bits into required positions.
This is most obvious if we write multiplication the long way - as we all did in school.
Let's say, for simplicity, that we deal with 4 bits, and want to extract them to 4 bytes.
Practically it means extracting each bit and placing it into first position of each of the 4 bytes.</p>
<p>Let's write 4 bit value as <code>dcba</code> (in little-endian bit order). Each letter represents bit value (either 1, or 0).</p>
<p>Right now I will write a series of operations that correspond to this code <code>((bits * magic) &amp; mask) &gt;&gt; shift</code>.
After that I will explain what happened.</p>
<pre><code>                               dcba *
    1000      100       10        1 = &lt;- magic
 dcba000   dcba00    dcba0     dcba &amp; 
    1000     1000     1000     1000 = &lt;- mask
    a        b        c        d    &gt;&gt; 
                                  3 = &lt;- shift
       a        b        c        d
</code></pre>
<p>Result of described operations in what we want - but with reversed order. We'll deal with order later, right now we want to understand what happened.
Each octet in <code>magic</code> has only 1 bit set. When we multiply octet, containing input bits by this magic number we effectively copy the input bits into each octet, shifted left.
Thus, when we mask the result of previous operation with correct mask, we are able to select different bits in each octet.
After that we only have to put these bits into correct positions.</p>
<p>The code for 8 bits is the following:</p>
<pre><code class="language-cpp">void unpack(uint8_t bits, bool bools[8]) {
	uint64_t magic = 0x8040201008040201ul;
	uint64_t mask  = 0x8080808080808080ul;
	uint64_t t = ((magic * bits) &amp; mask) &gt;&gt; 7;
	memcpy(bools, &amp;t, sizeof(t)); // NOT CORRECT!!!!!!
}
</code></pre>
<p>There is only one problem - the order of bytes is reversed. We can mitigate that using <code>BSWAP</code> <a href="https://www.felixcloutier.com/x86/bswap">instruction</a>, but it is quite slow (2 latency, 1 throughput).</p>
<pre><code class="language-cpp">void unpack(uint8_t bits, bool bools[8]) {
	uint64_t magic = 0x8040201008040201ul;
	uint64_t mask  = 0x8080808080808080ul;
	uint64_t t = ((magic * bits) &amp; mask) &gt;&gt; 7;
    t = _bswap64(t);
	memcpy(bools, &amp;t, sizeof(t)); 
}
</code></pre>
<p>So let's just reverse the bytes in <code>magic</code>! Unfortunately, that does not work as expected (I won't write whole expression):</p>
<pre><code>                           efghdcba *
00010000 00100000 01000000 10000000 = &lt;- magic
                   efghdcb a
           efghdc ba 
   efghd cba
</code></pre>
<p>We can see that during computation of multiplication certain bits overlap and thus get added up and carried - we do not get the 'parallel' multiplication as we did previously, when there was no overlap.
However, we can see that overlap always occurs with two bits: bit <code>a</code> and bit <code>e</code>. We know for a fact in which location bit <code>a</code> should go - in the first byte. We can mask this bit out and perform multiplication with <code>efghdcb0</code>. This way
no carry will occur, but bit <code>a</code> should be set in correct position by hand.</p>
<pre><code class="language-cpp">void unpack(uint8_t bits, bool bytes[8]) {
	uint64_t magic = 0x0102040810204080ul &gt;&gt; 7;
	uint64_t mask  = 0x0101010101010101ul;
	uint64_t lo = ((magic * (uint64_t)(bits &amp; 0xfe))) | bits;
	uint64_t result = lo &amp; mask;
	memcpy(bytes, &amp;result, sizeof(result));
}
</code></pre>
<blockquote>
<p>Here shift is done right in <code>magic</code>. Because lower octet is empty (as first bit would go there, but masked it out), it is valid.</p>
</blockquote>
<h3 id="benchmarks-1"><a class="header" href="#benchmarks-1">Benchmarks</a></h3>
<p>I originally stumbled upon this task when vectorizing loops that do comparison:</p>
<pre><code class="language-cpp">void compare32(const int32_t *a, const int32_t *b, bool *result, int nvalues) {
	for (int i = 0; i &lt; nvalues; ++i)
		result[i] = a[i] &gt; b[i];
}
</code></pre>
<p>Their AVX vector versions using bithack and <code>PDEP</code>:</p>
<pre><code class="language-cpp">void avx_compare32_w8_bithack(const int32_t *a, const int32_t *b, bool *result, int nvalues) {   
    #pragma GCC unroll 4
    for (int i = 0; i &lt; nvalues - 7; i += 8) {
        __m256i cmp1 = _mm256_cmpgt_epi32(_mm256_loadu_si256((__m256i *)(a + i)), 
                                          _mm256_loadu_si256((__m256i *)(b + i)));
        unsigned mask = (unsigned)_mm256_movemask_ps(_mm256_castsi256_ps(cmp1));
        unpack8bools(mask, result + i); 
    }
}

void avx_compare32_w8_bmi(const int32_t *a, const int32_t *b, bool *result, int nvalues) {   
    #pragma GCC unroll 4
    for (int i = 0; i &lt; nvalues - 7; i += 8) {
        __m256i cmp1 = _mm256_cmpgt_epi32(_mm256_loadu_si256((__m256i *)(a + i)), 
                                          _mm256_loadu_si256((__m256i *)(b + i)));
        unsigned mask = (unsigned)_mm256_movemask_ps(_mm256_castsi256_ps(cmp1));
        uint64_t combined = _pdep_u64(mask, 0x0101010101010101);
        memcpy(result + i, &amp;combined, sizeof(combined));
    }
}
</code></pre>
<p>I did benchmarks, and also benchmarked versions of these loops that use more vector registers per iteration: 2 and 4. These use have tasks of unpacking 16 bit to 16 bytes and 32 bits to 32 bytes respectively. These tasks can be solved using other techniques.</p>
<blockquote>
<p>I won't provide complete code here for other versions, but you can look <a href="https://stackoverflow.com/questions/48811369/how-to-use-bits-in-a-byte-to-set-dwords-in-ymm-register-without-avx2-inverse-o">here</a> for ideas</p>
</blockquote>
<p>These are the results:</p>
<div class="table-wrapper"><table><thead><tr><th>w32</th><th>w16</th><th>w8 bithack</th><th>w8 pdep</th></tr></thead><tbody>
<tr><td>78ns</td><td>99ns</td><td>158ns</td><td>105 ns</td></tr>
</tbody></table>
</div>
<p>We see that <code>PDEP</code> version performs better than bithack.
But bithack performs pretty well for such simple solution.</p>
<p>But the main point here is that using other number of vector registers allows us to avoid this task altogether, producing faster code on the way.</p>
<p>Anyway, I think this was an interesting problem to investigate, despite being useless on practice, as there are easy ways to avoid this problem. Extracting 16, 32, or 64 bits to bytes is much less problematic. And this problem is completely absent when using AVX-512 as there is <code>VMOVDQU8</code> <a href="https://www.felixcloutier.com/x86/movdqu:vmovdqu8:vmovdqu16:vmovdqu32:vmovdqu64">instruction</a>, which allows zero-masking, producing the same result.</p>
<h2 id="references"><a class="header" href="#references">References</a></h2>
<p>The idea is described <a href="https://stackoverflow.com/questions/14547087/extracting-bits-with-a-single-multiplication">here</a>.
Basic version of code is <a href="https://stackoverflow.com/questions/8461126/how-to-create-a-byte-out-of-8-bool-values-and-vice-versa/51750902#51750902">here</a>, but it does not work for the little-endian as expected (meaning we can't recast result value correctly to bool).</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>

        <!-- Livereload script (if served using the cli tool) -->
        <script>
            const wsProtocol = location.protocol === 'https:' ? 'wss:' : 'ws:';
            const wsAddress = wsProtocol + "//" + location.host + "/" + "__livereload";
            const socket = new WebSocket(wsAddress);
            socket.onmessage = function (event) {
                if (event.data === "reload") {
                    socket.close();
                    location.reload();
                }
            };

            window.onbeforeunload = function() {
                socket.close();
            }
        </script>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
